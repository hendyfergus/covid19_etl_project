[core]
# The home folder for Airflow
airflow_home = /opt/airflow

# The folder where your Airflow DAGs live
dags_folder = /opt/airflow/dags

# The folder where Airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3 or Google Cloud Storage. This
# is where you define the connection information
remote_logging = False

# The executor class that Airflow should use
executor = LocalExecutor

# The IP address and port for the web server to listen on
web_server_host = 0.0.0.0
web_server_port = 8080

# The secret key for secure connections to the web server
secret_key = my_secret_key

# The SQLAlchemy connection string for the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

[webserver]
# The number of worker processes for the web server
workers = 4

# The maximum number of requests a worker will process before restarting
max_requests = 5000

# The number of seconds to wait for requests before shutting down the worker
timeout = 60

[logging]
# The location of the Airflow log files
base_log_folder = /opt/airflow/logs

# The format for log messages
log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s

# The level of log messages to display (DEBUG, INFO, WARNING, ERROR, CRITICAL)
logging_level = INFO
